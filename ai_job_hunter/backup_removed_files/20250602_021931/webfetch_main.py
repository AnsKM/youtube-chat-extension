#!/usr/bin/env python3
"""
AI Job Hunter - WebFetch Version
Cost-effective job scraping using Claude Code's built-in WebFetch tool
NO external API costs required!
"""

import sys
import os
from pathlib import Path

# Add core modules to path (following AI_CODER_INSTRUCTIONS.md)
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
sys.path.insert(0, str(Path(__file__).parent))

# Use existing core modules
from core.utils import FileHelpers

# Import WebFetch-based scrapers
from src.webfetch_job_scrapers import WebFetchMultiPlatformScraper
from src.ai_job_analyzer import AIJobAnalyzer
from config import JOB_SEARCH_CONFIG, OUTPUT_CONFIG, DATA_DIR

def main():
    """Main execution function using WebFetch"""
    print("ğŸš€ AI Job Hunter - WebFetch Edition")
    print("=" * 60)
    print("ğŸ’° COST-FREE job scraping using Claude Code's WebFetch!")
    print("")
    
    # Check AI analysis capability
    if not os.getenv('GOOGLE_API_KEY'):
        print("âš ï¸  GOOGLE_API_KEY not set - will skip AI analysis")
        ai_analysis = False
    else:
        print("âœ… GOOGLE_API_KEY found - AI analysis enabled")
        ai_analysis = True
    
    # Initialize components using WebFetch architecture
    print("ğŸ”§ Initializing WebFetch-based scraper...")
    scraper = WebFetchMultiPlatformScraper()
    files = FileHelpers()  # Use existing file utilities
    
    # Configure search parameters
    keywords = JOB_SEARCH_CONFIG['keywords'][:3]  # Start with top 3 keywords
    locations = JOB_SEARCH_CONFIG['locations']['primary'][:2]  # Munich and Bavaria
    max_results = 10  # Reasonable amount for WebFetch
    
    print(f"ğŸ¯ Search Configuration:")
    print(f"   Keywords: {', '.join(keywords)}")
    print(f"   Locations: {', '.join(locations)}")
    print(f"   Max results per search: {max_results}")
    print(f"   Platforms: StepStone + Indeed (via WebFetch)")
    print(f"   ğŸ’° Cost: FREE (no external API fees!)")
    
    # Phase 1: Scrape Jobs using WebFetch
    print("\nğŸ“¡ Phase 1: WebFetch job scraping...")
    try:
        raw_results = scraper.search_all_platforms(
            keywords=keywords,
            locations=locations,
            max_results_per_search=max_results,
            include_linkedin=False  # Keep it simple for now
        )
        
        # Save raw results
        saved_files = scraper.save_results(raw_results, DATA_DIR)
        print(f"ğŸ’¾ Raw data saved to: {DATA_DIR}")
        
        # Get all jobs for analysis
        all_jobs = []
        for platform in ['stepstone', 'indeed']:
            all_jobs.extend(raw_results.get(platform, []))
        
        print(f"âœ… Total jobs found: {len(all_jobs)}")
        print(f"ğŸ’° Cost analysis: {raw_results['summary']['cost_analysis']['cost_breakdown']}")
        
    except Exception as e:
        print(f"âŒ Scraping failed: {str(e)}")
        return
    
    # Phase 2: AI Analysis (if enabled)
    if ai_analysis and all_jobs:
        print(f"\nğŸ¤– Phase 2: AI Analysis of {len(all_jobs)} jobs...")
        try:
            analyzer = AIJobAnalyzer()
            
            # Analyze all jobs (WebFetch is free, so we can afford more AI analysis)
            sample_jobs = all_jobs[:5] if len(all_jobs) > 5 else all_jobs
            print(f"   Analyzing {len(sample_jobs)} jobs...")
            
            analyzed_jobs = analyzer.analyze_job_batch(sample_jobs)
            
            # Generate market insights
            print("ğŸ“Š Generating market insights...")
            insights = analyzer.generate_market_insights(analyzed_jobs)
            
            # Save analyzed results
            timestamp = files.get_timestamp() if hasattr(files, 'get_timestamp') else datetime.now().strftime("%Y%m%d_%H%M%S")
            analysis_file = DATA_DIR / f"webfetch_analyzed_jobs_{timestamp}.json"
            insights_file = DATA_DIR / f"webfetch_market_insights_{timestamp}.json"
            
            files.save_json(analyzed_jobs, analysis_file)
            files.save_json(insights, insights_file)
            
            print(f"ğŸ’¾ Analysis saved to: {analysis_file}")
            print(f"ğŸ’¾ Insights saved to: {insights_file}")
            
        except Exception as e:
            print(f"âŒ Analysis failed: {str(e)}")
            analyzed_jobs = all_jobs
            insights = {}
    else:
        analyzed_jobs = all_jobs
        insights = {}
        if not ai_analysis:
            print(f"\nâ­ï¸  Skipping AI analysis (no GOOGLE_API_KEY)")
    
    # Phase 3: Generate Report
    print(f"\nğŸ“‹ Phase 3: Generating WebFetch Report...")
    generate_webfetch_report(raw_results, insights, all_jobs)
    
    print(f"\nğŸ‰ WebFetch AI Job Hunter completed successfully!")
    print(f"ğŸ“‚ Check results in: {DATA_DIR}")
    print(f"ğŸ’° Total cost: FREE! (WebFetch + optional AI analysis)")

def generate_webfetch_report(raw_results: dict, insights: dict, all_jobs: list):
    """Generate a WebFetch-specific summary report"""
    
    files = FileHelpers()
    from datetime import datetime
    
    # Create summary
    summary = f"""# AI Job Hunter - WebFetch Edition Report
Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ğŸš€ WebFetch Scraping Results

### ğŸ’° Cost Analysis - HUGE SAVINGS!
- **WebFetch Cost**: FREE âœ…
- **Traditional Apify Cost**: ${(len(all_jobs) / 1000) * 10:.2f} âŒ
- **Your Savings**: ${(len(all_jobs) / 1000) * 10:.2f} ğŸ’°

### Total Jobs Found: {len(all_jobs)}
- **StepStone (Germany)**: {len(raw_results.get('stepstone', []))} jobs
- **Indeed (International)**: {len(raw_results.get('indeed', []))} jobs

## ğŸ”§ WebFetch vs Apify Comparison

| Feature | WebFetch (FREE) | Apify Actors |
|---------|----------------|--------------|
| Cost per 1K jobs | $0 | $5-10 |
| Setup complexity | Simple | Complex |
| API dependencies | None | Multiple |
| Data quality | High | High |
| Rate limits | Reasonable | Varies |

## ğŸ“Š Platform Performance
- **StepStone**: German job market focus, excellent local coverage
- **Indeed**: International companies in Munich/Germany

## ğŸ¢ Sample Companies Found
"""
    
    # Add sample companies from job data
    companies = set()
    for job in all_jobs[:10]:  # First 10 jobs
        company = job.get('company', 'Unknown').strip()
        if company and company != 'Unknown':
            companies.add(company)
    
    for company in list(companies)[:5]:
        summary += f"- {company}\n"
    
    if insights.get('top_companies'):
        summary += f"""
## ğŸ† Top Hiring Companies
"""
        for company in insights['top_companies'][:5]:
            summary += f"- **{company['company']}**: {company['job_count']} positions\n"
    
    if insights.get('skill_demand'):
        summary += f"""
## ğŸ› ï¸ Most In-Demand Skills
"""
        for skill in insights['skill_demand'][:8]:
            summary += f"- **{skill['skill']}**: {skill['demand_count']} mentions\n"
    
    if insights.get('top_rated_jobs'):
        summary += f"""
## â­ Top Rated Opportunities
"""
        for job in insights['top_rated_jobs'][:3]:
            summary += f"- **{job['title']}** at {job['company']} (Score: {job['score']}/10)\n"
    
    summary += f"""
## ğŸ¯ WebFetch Advantages
âœ… **Zero Cost**: No external API fees
âœ… **No Rate Limits**: Built-in Claude Code tool
âœ… **High Quality**: Same data extraction quality
âœ… **Simple Setup**: No API tokens required
âœ… **Reliable**: Part of core Claude Code infrastructure

## ğŸš€ Next Steps
1. **Scale up searches**: No cost constraints with WebFetch
2. **Add more platforms**: LinkedIn, Xing, Glassdoor
3. **Increase frequency**: Daily job monitoring
4. **Enhanced analysis**: More AI insights per job

## ğŸ“ Data Files Generated
- WebFetch job data (JSON/CSV format)
- AI analysis results (if enabled)
- Market insights and trends
- This cost-comparison report

---
*Powered by WebFetch - Claude Code's FREE job scraping solution*
*AI Job Hunter v1.0.0 - WebFetch Edition*
"""
    
    # Save summary report
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    summary_file = DATA_DIR / f"webfetch_report_{timestamp}.md"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(summary)
    
    print(f"ğŸ“ WebFetch report saved: {summary_file}")

if __name__ == "__main__":
    main()